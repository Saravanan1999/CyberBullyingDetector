{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"en_hf_112024.csv\")  # Update this path if needed\n",
    "df = df.iloc[:10000]\n",
    "# Load model & tokenizer\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare pipeline\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NON_HATE', 1: 'HATE'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "print(config.id2label)\n",
    "# preds = []\n",
    "# for text in tqdm(df[\"text\"].astype(str).tolist()):\n",
    "#     pred = pipeline(text, truncation=True, max_length=512)[0]\n",
    "#     label = 1 if pred[\"label\"] == \"HATE\" else 0  # âœ… CORRECT now\n",
    "#     preds.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Accuracy:  0.2157\n",
      "Precision: 0.2264\n",
      "Recall:    0.6021\n",
      "F1 Score:  0.3290\n"
     ]
    }
   ],
   "source": [
    "true_labels = df[\"labels\"].tolist()\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "precision = precision_score(true_labels, preds, zero_division=0)\n",
    "recall = recall_score(true_labels, preds, zero_division=0)\n",
    "f1 = f1_score(true_labels, preds, zero_division=0)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(true_labels, preds)\n\u001b[1;32m      3\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, preds, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "true_labels = df[\"Label\"].tolist()\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "precision = precision_score(true_labels, preds, zero_division=0)\n",
    "recall = recall_score(true_labels, preds, zero_division=0)\n",
    "f1 = f1_score(true_labels, preds, zero_division=0)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'non-hate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate\")\n",
    "print(config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " Label\n",
      "1    364525\n",
      "0    361594\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:23<00:00, 24239.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:05<00:00, 25575.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:00<00:00, 1126184.84 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:00<00:00, 1257177.07 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Hate-speech-CNERG/dehatebert-mono-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [01:36<00:00, 6048.55 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:23<00:00, 6069.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define Preprocessing Function\n",
    "# -------------------------\n",
    "def basic_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by lowercasing, removing URLs and usernames,\n",
    "    and stripping extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)              # Remove usernames\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)           # Remove punctuation (optional)\n",
    "    return text.strip()\n",
    "\n",
    "def convert_labels(batch):\n",
    "    # Convert each label to float explicitly.\n",
    "    batch[\"Label\"] = [float(label) for label in batch[\"Label\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load Your Dataset\n",
    "# -------------------------\n",
    "# Update the path to your CSV file (the hate superset CSV)\n",
    "csv_file = \"HateSpeechDatasetBalanced.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df[\"Content\"] = df[\"Content\"].astype(str).apply(basic_preprocessing)\n",
    "\n",
    "# Optional: Check label distribution\n",
    "print(\"Label distribution:\\n\", df[\"Label\"].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split the Dataset into Train and Test\n",
    "# -------------------------\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Label\"])\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict with train and test splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# 4. Load the HateBERT Tokenizer and Tokenize the Dataset\n",
    "# -------------------------\n",
    "# Set your HateBERT model name (change if needed)\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Content\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize datasets in batch\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Prepare for PyTorch\n",
    "# -------------------------\n",
    "# Ensure the label column is called \"labels\" (if not, rename it)\n",
    "# Note: The CSV should already have a \"labels\" column.\n",
    "# Convert to int if needed\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n",
    "# tokenized_datasets = tokenized_datasets.map(lambda x: {\"Label\": np.float32(x[\"Label\"])})\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"post_id\", \"post_tokens\", \"annotators\", \"rationales\"]) if \"post_id\" in tokenized_datasets[\"train\"].column_names else tokenized_datasets\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Load the HateBERT Model for Sequence Classification\n",
    "# -------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Define Training Arguments and Metrics\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hatebert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))  # sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "tokenized_datasets = tokenized_datasets.map(lambda x: {\"labels\": x[\"labels\"].float()})\n",
    "\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(sample[\"labels\"], type(sample[\"labels\"]))\n",
    "# -------------------------\n",
    "# 8. Initialize the Trainer and Train the Model\n",
    "# -------------------------\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # -------------------------\n",
    "# # 9. Evaluate the Model\n",
    "# # -------------------------\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " Label\n",
      "1    364525\n",
      "0    361594\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:23<00:00, 24239.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:05<00:00, 25575.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:00<00:00, 1126184.84 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:00<00:00, 1257177.07 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Hate-speech-CNERG/dehatebert-mono-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [01:36<00:00, 6048.55 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:23<00:00, 6069.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define Preprocessing Function\n",
    "# -------------------------\n",
    "def basic_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by lowercasing, removing URLs and usernames,\n",
    "    and stripping extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)              # Remove usernames\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)           # Remove punctuation (optional)\n",
    "    return text.strip()\n",
    "\n",
    "def convert_labels(batch):\n",
    "    batch[\"Label\"] = [np.float32(label) for label in batch[\"Label\"]]\n",
    "    return batch\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load Your Dataset\n",
    "# -------------------------\n",
    "# Update the path to your CSV file (the hate superset CSV)\n",
    "csv_file = \"HateSpeechDatasetBalanced.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df[\"Content\"] = df[\"Content\"].astype(str).apply(basic_preprocessing)\n",
    "\n",
    "# Optional: Check label distribution\n",
    "print(\"Label distribution:\\n\", df[\"Label\"].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split the Dataset into Train and Test\n",
    "# -------------------------\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Label\"])\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict with train and test splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# 4. Load the HateBERT Tokenizer and Tokenize the Dataset\n",
    "# -------------------------\n",
    "# Set your HateBERT model name (change if needed)\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Content\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize datasets in batch\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Prepare for PyTorch\n",
    "# -------------------------\n",
    "# Ensure the label column is called \"labels\" (if not, rename it)\n",
    "# Note: The CSV should already have a \"labels\" column.\n",
    "# Convert to int if needed\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n",
    "# tokenized_datasets = tokenized_datasets.map(lambda x: {\"Label\": np.float32(x[\"Label\"])})\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"post_id\", \"post_tokens\", \"annotators\", \"rationales\"]) if \"post_id\" in tokenized_datasets[\"train\"].column_names else tokenized_datasets\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Load the HateBERT Model for Sequence Classification\n",
    "# -------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Define Training Arguments and Metrics\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hatebert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))  # sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "tokenized_datasets = tokenized_datasets.map(lambda x: {\"labels\": x[\"labels\"].float()})\n",
    "\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(sample[\"labels\"], type(sample[\"labels\"]))\n",
    "# -------------------------\n",
    "# 8. Initialize the Trainer and Train the Model\n",
    "# -------------------------\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # -------------------------\n",
    "# # 9. Evaluate the Model\n",
    "# # -------------------------\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " Label\n",
      "1.0    364525\n",
      "0.0    361594\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:24<00:00, 23921.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:05<00:00, 24481.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580895/580895 [00:00<00:00, 972637.15 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145224/145224 [00:00<00:00, 1445088.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label: tensor(1.) <class 'torch.Tensor'> torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define Preprocessing Function\n",
    "# -------------------------\n",
    "def basic_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by lowercasing, removing URLs and usernames,\n",
    "    and stripping extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)              # Remove usernames\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)           # Remove punctuation (optional)\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load and Preprocess Dataset\n",
    "# -------------------------\n",
    "csv_file = \"HateSpeechDatasetBalanced.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert the label column to float32 right away.\n",
    "df[\"Label\"] = df[\"Label\"].astype(np.float32)\n",
    "df[\"Content\"] = df[\"Content\"].astype(str).apply(basic_preprocessing)\n",
    "\n",
    "# Optional: Check label distribution\n",
    "print(\"Label distribution:\\n\", df[\"Label\"].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# 3. Split Dataset into Train and Test\n",
    "# -------------------------\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Label\"])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# 4. Tokenize the Dataset\n",
    "# -------------------------\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Content\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Ensure Labels are float32\n",
    "# -------------------------\n",
    "# First, rename the column to \"labels\"\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Label\", \"labels\")\n",
    "# Then, force conversion using numpy to ensure the type is float32\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda x: {\"labels\": np.array(x[\"labels\"], dtype=np.float32).tolist()},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Finally, set the format for PyTorch\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Verify the conversion\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(\"Sample label:\", sample[\"labels\"], type(sample[\"labels\"]), sample[\"labels\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Hate-speech-CNERG/dehatebert-mono-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/sy/j85r4wzj1b76bxxb72ht66cr0000gn/T/ipykernel_28781/112935248.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108918' max='108918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108918/108918 18:03:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.107191</td>\n",
       "      <td>0.759089</td>\n",
       "      <td>0.744953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.833113</td>\n",
       "      <td>0.829384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.106646</td>\n",
       "      <td>0.838174</td>\n",
       "      <td>0.834827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.10664601624011993, 'eval_accuracy': 0.8381741309976313, 'eval_f1': 0.8348267146303472, 'eval_runtime': 4657.276, 'eval_samples_per_second': 31.182, 'eval_steps_per_second': 1.949, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, ignore_mismatched_sizes=True)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Define Training Arguments and Metrics\n",
    "# -------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hatebert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Apply sigmoid since we're in a binary (regression-like) setup\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# -------------------------\n",
    "# 8. Initialize the Trainer and Train the Model\n",
    "# -------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------\n",
    "# 9. Evaluate the Model\n",
    "# -------------------------\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      1.00      0.66     24873\n",
      "         1.0       0.00      0.00      0.00     25127\n",
      "\n",
      "    accuracy                           0.50     50000\n",
      "   macro avg       0.25      0.50      0.33     50000\n",
      "weighted avg       0.25      0.50      0.33     50000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Step 1: Sample 10,000 random indices from eval dataset\n",
    "eval_dataset = trainer.eval_dataset\n",
    "sample_size = min(50000, len(eval_dataset))\n",
    "random_indices = random.sample(range(len(eval_dataset)), sample_size)\n",
    "\n",
    "# Step 2: Create a subset\n",
    "sampled_eval_dataset = Subset(eval_dataset, random_indices)\n",
    "\n",
    "# Step 3: Run prediction on the subset\n",
    "sampled_output = trainer.predict(sampled_eval_dataset)\n",
    "preds = np.argmax(sampled_output.predictions, axis=1)\n",
    "labels = sampled_output.label_ids\n",
    "\n",
    "# Step 4: Compute confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "# Step 5: Plot it\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Sample of 10,000)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw logits:\n",
      " [[-0.02984716]\n",
      " [ 0.00765086]\n",
      " [-0.01901553]\n",
      " [ 0.42898566]\n",
      " [ 0.41167766]]\n",
      "\n",
      "Softmax probabilities:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "Predicted labels: [0 0 0 0 0]\n",
      "True labels:      [1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "sample_output = trainer.predict(trainer.eval_dataset.select(range(5)))\n",
    "\n",
    "# Raw logits\n",
    "logits = sample_output.predictions\n",
    "print(\"Raw logits:\\n\", logits)\n",
    "\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "print(\"\\nSoftmax probabilities:\\n\", probs)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "print(\"\\nPredicted labels:\", preds)\n",
    "\n",
    "true_labels = sample_output.label_ids\n",
    "print(\"True labels:     \", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     24873\n",
      "           1       0.50      1.00      0.67     25127\n",
      "\n",
      "    accuracy                           0.50     50000\n",
      "   macro avg       0.25      0.50      0.33     50000\n",
      "weighted avg       0.25      0.50      0.34     50000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[    0 24873]\n",
      " [    0 25127]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stygianphantom/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions on the eval dataset\n",
    "preds_output = trainer.predict(sampled_eval_dataset)\n",
    "logits = preds_output.predictions\n",
    "labels = preds_output.label_ids\n",
    "\n",
    "# Apply sigmoid for binary classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.86     24873\n",
      "           1       0.83      0.95      0.88     25127\n",
      "\n",
      "    accuracy                           0.87     50000\n",
      "   macro avg       0.88      0.87      0.87     50000\n",
      "weighted avg       0.88      0.87      0.87     50000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19940  4933]\n",
      " [ 1336 23791]]\n"
     ]
    }
   ],
   "source": [
    "probs = 1 / (1 + np.exp(-logits))\n",
    "preds = (probs > 0.51).astype(int)\n",
    "\n",
    "# Flatten in case logits are shaped (N, 1)\n",
    "preds = preds.flatten()\n",
    "labels = labels.astype(int)\n",
    "\n",
    "# Recompute metrics\n",
    "print(classification_report(labels, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hate_speech_recall_optimized_model/tokenizer_config.json',\n",
       " 'hate_speech_recall_optimized_model/special_tokens_map.json',\n",
       " 'hate_speech_recall_optimized_model/vocab.txt',\n",
       " 'hate_speech_recall_optimized_model/added_tokens.json',\n",
       " 'hate_speech_recall_optimized_model/tokenizer.json')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"hate_speech_recall_optimized_model\")\n",
    "\n",
    "# Save tokenizer too (important!)\n",
    "tokenizer.save_pretrained(\"hate_speech_recall_optimized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/j85r4wzj1b76bxxb72ht66cr0000gn/T/ipykernel_28781/2324367296.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Step 1: Load model and tokenizer\n",
    "model_path = \"hate_speech_recall_optimized_model\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Step 2: Re-initialize Trainer (must pass same eval_dataset!)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=sampled_eval_dataset  # or full eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  18/6250 00:01 < 08:15, 12.57 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get predictions on the eval dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m preds_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_eval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m logits \u001b[38;5;241m=\u001b[39m preds_output\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m preds_output\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:4183\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4180\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4182\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4183\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   4185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4186\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:4321\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4319\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m   4320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4321\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4323\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:2713\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/utils/operations.py:408\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    410\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/utils/operations.py:678\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    675\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/utils/operations.py:658\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    655\u001b[0m     dim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    659\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions on the eval dataset\n",
    "preds_output = trainer.predict(sampled_eval_dataset)\n",
    "logits = preds_output.predictions\n",
    "labels = preds_output.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
