# CyberBullyingCombat

Hate speech, cyberbullying and offensive language are distinct concepts but are closely interconnected with each other. When offensive language/extreme hatred is targeted towards a particular individual with malicious intent, it becomes cyberbullying. This is a pressing issue that many individuals face with no way to escape. There are many cases where individuals resort to suicide as a consequence of cyberbullying. Various laws and policies have been put in place to curb this issue but it still persists. The objective of our project is to develop a tool that can detect cyberbullying/hate speech/offensive language in their tweets or comments or posts on social media. Through this project, we aim to inspire social media platforms and other governmental organizations to create advanced detection frameworks that can prevent cyberbullying before they cause harm. 

This section follows a literature review of a few previous articles related to the topic at hand. Beroud et al. (2017-2021) reviewed automated methods for detecting abusive content on social media using NLP, highlighting the need for context-aware models, multilingual support, and recognizing language subtleties. In 2023, Sinyangwe et al. developed a model to detect hate speech and offensive language on platforms like Twitter and Facebook, using agile methods, deep learning, and the HateSonar tool. They trained the model on English and tested it on a local language, finding offensive language globally prevalent. The model effectively identified offensive content and improved data categorization. Lorens et al. used ML techniques in their dissertation to detect abusive content on Twitter, comparing models with metrics like F-1 score, recall, and precision. They found ML models successfully predicted hate speech and offensive language, with some outperforming others in accuracy.

As we can see, all these studies focus on accurately predicting offensive language and hate speech, but they give less emphasis to model interpretability. This observation raised an important question we were interested in exploring: can we prioritize both interpretability and accuracy? In order to achieve this, we have chosen the following methods to conduct our research.

Identifying hate words and their contextual meanings is essential for understanding cyberbullying sentence structures, as cyberbullying often incorporates explicit insults, threats, and demeaning language that directly align with hate speech. However, cyberbullying can also be indirect, sarcastic, or non-explicit, which is why analyzing structural patterns - such as sentence formation, attack style, and repeated harassment - is crucial. These patterns help the model capture subtle nuances that might be overlooked when focusing only on explicit vocabulary. To address this, we make use of two types of data: hate speech datasets and cyberbullying datasets. Hate speech datasets sourced from platforms like Github, Hugging Face, Hate Speech Data, provide a solid foundation on toxic vocabulary, enabling the model to understand both explicit hate words and their subtle contextual meaning. Cyberbullying datasets sourced from Mendeley Data, Kaggle(two datasets), and Github capture the real-world application of hate speech in bullying scenarios, helping the model learn the sentence structures and behavioral patterns involved in repeated harassment and targeting individuals. This dual-data approach ensures a comprehensive understanding and enhances the overall accuracy and interpretability of our detection system. 

To provide a detailed view of our approach, we will perform two types of feature engineering: lexicon-based and structure-based. For the lexicon-based component, we will employ TF-IDF and N-gram analysis to identify key phrases and detect implicit hate words, while lexicon word matching will count the occurrence of hate words within a sentence. On the structure-based side, dependency parsing and POS tagging will be used to pinpoint common cyberbullying sentence structures, and syntactic pattern extraction with BERT-based sentence embeddings will help uncover similar toxic constructions. Additionally, sentiment analysis using VADER or TextBlob will capture the emotional context behind each comment, ensuring that both the explicit language and underlying sentiment are thoroughly analyzed.

Model training will adopt a multi-staged approach that leverages both traditional machine learning and deep learning techniques. Initially, feature learning will be carried out using XGBoost or Logistic Regression to detect specific hate words. Next, to identify sentence patterns, we will apply CNNs or LSTMs that focus on structural cues of cyberbullying. For capturing the subtle nuances of context - such as sarcasm or indirect hatred - a fine-tuned BERT model will be employed to understand everyday language complexities. Finally, SHAP and LIME will be integrated to interpret the model's decisions and provide clear explanations of why a comment was flagged, ensuring transparency and accountability in our detection system.

What makes this project unique and creative is its multi-layered approach that combines traditional machine learning, deep learning, and XAI techniques to not only detect cyberbullying but also provide transparency into its decision-making process. By integrating lexicon-based feature learning, structural pattern recognition and contextual understanding with different ML and DL techniques, the system captures both explicit hate speech and subtle, implicit bullying behaviors. The use of XAI methods - an emerging research area - sheds light on the rationale behind flagging comments, enhancing interpretability by pinpointing which words contributed most to a comment being tagged as offensive. Additionally, the future scope of this project includes narrowing its focus towards specific underrepresented groups, as our current efforts address the general public rather than individual groups. 

In summary, our approach is motivated by the need to address the accurate prediction of offensive language and hate speech and also the crucial aspect of interpretability, which has been underemphasized in previous studies. By combining diverse datasets with advanced feature engineering and a multi-staged training process using traditional ML, deep learning, and XAI techniques, we aim to create a precise and transparent cyberbullying detection system that improves the well-being of our online community. 
